>>> from prepare import prepare
>>> prepare()
>>> from transformers import AutoModel, AutoTokenizer, AutoConfig
>>> config = AutoConfig.from_pretrained('bert-base-uncased')
>>> config.output_hidden_states
False
>>> config.output_hidden_states = True
>>> tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
>>> model = AutoModel.from_pretrained('bert-base-uncased', config=config)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
>>> tokenized_input = tokenizer('Imagine all the people', return_tensors='pt')
>>> res = model(**tokenized_input)
>>> res.keys()
odict_keys(['last_hidden_state', 'pooler_output', 'hidden_states'])
>>> res.last_hidden_state.shape
torch.Size([1, 6, 768])
>>> len(res.hidden_states)
13
>>> res.hidden_states[0].shape
torch.Size([1, 6, 768])
>>> res.hidden_states[1].shape
torch.Size([1, 6, 768])